{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321ef82c",
   "metadata": {},
   "source": [
    "# Feature Engineering Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc8826",
   "metadata": {},
   "source": [
    "1 What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20363f3d",
   "metadata": {},
   "source": [
    "SOLUTION 1 \n",
    "\n",
    "A parameter is the variable defined within the parentheses during function definition. \n",
    "Simply they are written when we declare a function.\n",
    "EXAMPLE\n",
    "def sum_sub(a,b,c):\n",
    "    return a+b-c\n",
    "a,b,c are parameter \n",
    "when we give value to a,b,c while recalling above function then they are called argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32080716",
   "metadata": {},
   "source": [
    "2 What is correlation?\n",
    "What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d63bf0",
   "metadata": {},
   "source": [
    "SOLUTION 2\n",
    "\n",
    "Correlation is a statistical term that describes how two variables move in relation to each other. \n",
    "A negative correlation, also known as an inverse correlation, means that two variables move in opposite directions. \n",
    "For example, as one variable increases, the other decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20757690",
   "metadata": {},
   "source": [
    "3 Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f66b4d",
   "metadata": {},
   "source": [
    "SOLUTION 3\n",
    "\n",
    "Machine learning (ML) is a type of artificial intelligence (AI) that allows computers to learn from data, find patterns, and make predictions with little human intervention. The main components of machine learning algorithms are:\n",
    "Representation\n",
    "How the model looks and how knowledge is represented. Decision trees and neural networks are examples of representation.\n",
    "Evaluation\n",
    "How good models are differentiated and how programs are evaluated. Prediction and recall are examples of evaluation methods.\n",
    "Optimization\n",
    "The process for finding good models and how programs are generated. Convex optimization is an example of an optimization process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3ffef",
   "metadata": {},
   "source": [
    "4 How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb8343",
   "metadata": {},
   "source": [
    "SOLUTION 4\n",
    "\n",
    "Loss value quantifying the difference (“loss”) between a predicted value—that is, the model's output—for a given input and the actual value or ground truth. If a model's predictions are accurate, the loss is small.\n",
    "If its predictions are inaccurate, the loss is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b44894",
   "metadata": {},
   "source": [
    "5 What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b664fc",
   "metadata": {},
   "source": [
    "SOLUTION 5\n",
    "\n",
    "Continuous variables can take any value within a range, while categorical variables are descriptive and have a fixed number of values:\n",
    "\n",
    "Continuous variables\n",
    "Can take any value between a theoretical minimum and maximum. They are often measured on a continuous scale, such as weight, height, or temperature. Continuous variables are expressed as absolute numbers for each subject in a sample. \n",
    "\n",
    "Categorical variables\n",
    "Have a fixed number of values and are descriptive, not numerical. They are expressed as category frequencies in a sample. Examples of categorical variables include hair color, gum flavor, dog breed, and cloud type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5905f",
   "metadata": {},
   "source": [
    "6 How do we handle categorical variables in Machine Learning? What are the common t\n",
    "echniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17e395",
   "metadata": {},
   "source": [
    "SOLUTION 6 \n",
    "\n",
    "Here are some techniques for handling categorical variables in machine learning:\n",
    "\n",
    "One-hot encoding\n",
    "Creates a separate binary column for each category. This technique can lead to high-dimensional feature spaces. \n",
    "\n",
    "Binary encoding\n",
    "Converts categories into binary digits. This is a hybrid approach that reduces the dimensionality created by one-hot encoding. \n",
    "\n",
    "Label encoding\n",
    "Assigns a unique numerical value to each category. This technique is useful for variables with a large number of categories. However, it can introduce ordinality into the data. \n",
    "\n",
    "Ordinal encoding\n",
    "Similar to label encoding, but allows you to explicitly define the mapping between categories and integer labels. This is useful when there is a clear and predefined ordinal relationship. \n",
    "\n",
    "Target encoding\n",
    "Converts a categorical value into the mean of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90394b94",
   "metadata": {},
   "source": [
    "7 What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f52880",
   "metadata": {},
   "source": [
    "SOLUTION 7\n",
    "\n",
    "Training and testing a dataset is a process used in machine learning to teach a model and evaluate its performance: \n",
    "\n",
    "Training\n",
    "The model is taught to recognize patterns or perform criteria using a subset of the data called the training dataset. The model adjusts its parameters based on the training data. The more training data a model has, the better it can make predictions. \n",
    "\n",
    "Testing\n",
    "The model's performance is evaluated using a subset of the data called the testing dataset. The testing dataset should be independent of the training dataset, meaning its contents should not have been used for training. The model's performance is evaluated on unseen data to check if its predictions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29977586",
   "metadata": {},
   "source": [
    "8 What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac93110",
   "metadata": {},
   "source": [
    "SOLUTION 8\n",
    "\n",
    "The sklearn. preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. In general, learning algorithms benefit from standardization of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd012399",
   "metadata": {},
   "source": [
    "9 What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a09c1f",
   "metadata": {},
   "source": [
    "‍SOLUTION 9\n",
    "\n",
    "The test set is a portion (or partition) of the available training data that is “held back” and not used during model training. The purpose of the test set is to evaluate the performance of the model on unseen data after it has been trained. As such, the test set should not be explicitly or implicitly used during training or hyperparameter tuning. The test set is typically 10-30% of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad37b5",
   "metadata": {},
   "source": [
    "10 How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394adb7",
   "metadata": {},
   "source": [
    "SOLUTION 10\n",
    "\n",
    "Here are a few common processes for splitting data:\n",
    "\n",
    "Train-Test Split: The dataset is divided right into a training set and a trying out set. The education set is used to educate the model, even as the checking out set  is used to assess the model’s overall performance. The regular cut up is 70-eighty% for training and 20-30% for checking out, but this may vary depending on the scale of the dataset and the precise use case.\n",
    "\n",
    "Train-Validation-Test Split: The dataset is split into three subsets – a schooling set, a validation set, and a trying out set. The training set is used to train the version, the validation set is used to tune hyperparameters and validate the version’s overall performance for the duration of training, and the testing set is used to evaluate the very last version’s overall performance.\n",
    "\n",
    "K-fold Cross Validation: The dataset is divided into ok equally sized folds, and the version is educated and evaluated okay instances. Each time, k-1 folds are used for training, and 1 fold is used for validation/testing. This allows in acquiring greater strong overall performance estimates and reduces the variance in version evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85aa400",
   "metadata": {},
   "source": [
    "11 Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe940b",
   "metadata": {},
   "source": [
    "SOLUTION 11\n",
    "\n",
    "Exploratory Data Analysis (EDA) is important before fitting a model to data because it helps you understand the data and identify issues that could affect the model's results:\n",
    "\n",
    "Identify issues: EDA can help you find errors like missing values, incorrect labels, outliers, or duplicates. \n",
    "\n",
    "Understand patterns: EDA can help you identify patterns and trends in the data. \n",
    "\n",
    "Guide analysis: EDA can help you prioritize areas for further analysis and model development. \n",
    "\n",
    "Inform feature selection: EDA can help you inform how to select and transform features. \n",
    "\n",
    "Improve results: EDA can help you gain insights and intuition about your data, which can improve your results. \n",
    "\n",
    "Ensure accurate analysis: EDA can help ensure that your analysis is accurate and reliable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46914e6e",
   "metadata": {},
   "source": [
    "12 What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee1bad0",
   "metadata": {},
   "source": [
    "SOLUTION 12\n",
    "\n",
    "Correlation in Python is a statistical tool that measures the strength and direction of the linear relationship between two variables. It's represented by a single number, the correlation coefficient, which is usually denoted as r. The correlation coefficient ranges from -1 to 1, with the following meanings:\n",
    "\n",
    "1: Perfect positive correlation, meaning the variables move together proportionally\n",
    "\n",
    "-1: Perfect negative correlation, meaning the variables move in opposite directions proportionally\n",
    "\n",
    "0: No linear relationship between the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48131a4",
   "metadata": {},
   "source": [
    "13 What does negative correlation mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a69fa",
   "metadata": {},
   "source": [
    "SOLUTION 13\n",
    "\n",
    "A negative correlation, also known as an inverse correlation, is a mathematical relationship between two variables that move in opposite directions: \n",
    "As one variable increases, the other decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51536eaf",
   "metadata": {},
   "source": [
    "14 How can you find correlation between variables in Python?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2f765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974894414261588\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#SOLUTION 14\n",
    "#BY FOLLOEWING METHOD WE CAN CALCULATE CORRELATION\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1,3,5,7,8,9, 10, 15])\n",
    "y = np.array([10, 20, 30, 40, 50, 60, 70, 80])\n",
    "def Pearson_correlation(X,Y):\n",
    "    if len(X)==len(Y):\n",
    "        Sum_xy = sum((X-X.mean())*(Y-Y.mean()))\n",
    "        Sum_x_squared = sum((X-X.mean())**2)\n",
    "        Sum_y_squared = sum((Y-Y.mean())**2)\t \n",
    "        corr = Sum_xy / np.sqrt(Sum_x_squared * Sum_y_squared)\n",
    "    return corr\n",
    "\n",
    "print(Pearson_correlation(x,y)) \n",
    "print(Pearson_correlation(x,x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901bcb9",
   "metadata": {},
   "source": [
    "15 What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5d7a7",
   "metadata": {},
   "source": [
    "SOLUTION 15\n",
    "\n",
    " Correlation :\n",
    "It is a statistical term which depicts the degree of association between two random variables. In data analysis it is often used to determine the amount to which they relate to one another.\n",
    "\n",
    "Causation :\n",
    "Causation between random variables A and B implies that A and B have a cause-and-effect relationship with one another. Or we can say existence of one gives birth to other, and we say A causes B or vice versa. Causation is also termed as causality.\n",
    "\n",
    "Correlation does not imply Causation.\n",
    "\n",
    "Correlation and Causation can exist at the same time also, so definitely correlation doesn’t imply causation. Below example is to show this difference more clearly-\n",
    "No battery in computer causes computer to shut and also causes video player to stop shows causality of battery over laptop and video player. The moment computer shuts, video player also shuts shows both are correlated. More specifically positively correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57976d7f",
   "metadata": {},
   "source": [
    "16 What is an Optimizer? What are different types of optimizers? Explain each with an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4208137",
   "metadata": {},
   "source": [
    "SOLUTION 16\n",
    "An optimizer is an algorithm or method that updates model parameters to reduce loss with less effort. Some common optimizers include: \n",
    "\n",
    "Gradient descent\n",
    "An algorithm that minimizes a function by iteratively adjusting its parameters. \n",
    "\n",
    "Stochastic gradient descent (SGD)\n",
    "An extension of gradient descent that improves on some of its disadvantages. \n",
    "\n",
    "Mini-batch gradient descent\n",
    "A variant of gradient descent that balances the efficiency of SGD with the stability of batch gradient descent. \n",
    "\n",
    "Momentum\n",
    "A technique that accelerates neural network training by adding a fraction of the previous update to the current update. \n",
    "\n",
    "RMSProp (Root Mean Square Propagation)\n",
    "An algorithm that adjusts the learning rate of each parameter based on its recent gradients. \n",
    "\n",
    "AdaGrad\n",
    "An optimizer that works with a global learning rate, which can cause slow convergence. \n",
    "\n",
    "SGD with momentum\n",
    "An algorithm that improves SGD's convergence speed and stability by reducing oscillations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd5a45",
   "metadata": {},
   "source": [
    "17 What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc503d",
   "metadata": {},
   "source": [
    "SOLUTION 17\n",
    "\n",
    "linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models. The term linear model implies that the model is specified as a linear combination of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8433e2",
   "metadata": {},
   "source": [
    "18 What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73cf3ba",
   "metadata": {},
   "source": [
    "SOLUTION 18\n",
    "\n",
    "The fit() method in Scikit-Learn is used to train a machine learning model. Training a model involves feeding it with data so it can learn the underlying patterns. This method adjusts the parameters of the model based on the provided data\n",
    "\n",
    "argument of model.fit() should be training dataset and not testing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108e7f4",
   "metadata": {},
   "source": [
    "19 What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553a863",
   "metadata": {},
   "source": [
    "SOLUTION 19\n",
    "\n",
    "Model. predict passes the input vector through the model and returns the output tensor for each datapoint. Since the last layer in your model is a single Dense neuron, the output for any datapoint is a single value. And since you didn't specify an activation for the last layer, it will default to linear activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ffceb",
   "metadata": {},
   "source": [
    "20 REPEATED QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7bd4b9",
   "metadata": {},
   "source": [
    "21 What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "21 What is feature scaling? How does it help in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e113f1f",
   "metadata": {},
   "source": [
    "SOLUTION 21\n",
    "\n",
    "Feature scaling is a crucial step in machine learning that transforms the numerical features in a dataset to a common scale. It's also known as data normalization. \n",
    "Feature scaling helps in machine learning by: \n",
    "\n",
    "Ensuring equal contribution\n",
    "By scaling features to a similar range, you ensure that each feature contributes equally to the model's final prediction. Without scaling, models might weigh features unequally, leading to poor performance. \n",
    "\n",
    "Easing convergence\n",
    "For gradient descent-based algorithms, feature scaling can speed up convergence by helping the optimization algorithm reach the minima faster. \n",
    "\n",
    "Creating a different model fit\n",
    "Feature scaling can create a completely different model fit compared to the fit with unscaled data. \n",
    "\n",
    "Enabling fair comparison\n",
    "Feature scaling transforms data into a standard scale, enabling fair comparison between different features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869b47c",
   "metadata": {},
   "source": [
    "22 How do we perform scaling in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c7705",
   "metadata": {},
   "source": [
    "SOLUTION 22\n",
    "To perform scaling in Python, you can use different techniques, including: \n",
    "\n",
    "Normalization\n",
    "Transforms feature values to a specific range, usually between 0 and 1. This method is useful when you want to maintain the relative distances between values while ensuring that all features are on the same scale. \n",
    "\n",
    "Standardization\n",
    "Centers the data, then divides by the standard deviation to enforce that the standard deviation of the variable is one. This technique is particularly useful when dealing with algorithms that assume normally distributed data. \n",
    "\n",
    "Robust scaling\n",
    "Rescales the values of each feature by removing the median and dividing by the interquartile range (IQR). Robust scaling is less sensitive to outliers compared to other scaling techniques. \n",
    "\n",
    "StandardScaler\n",
    "Scales the data such that the mean is 0 and the standard deviation is 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfefb1e",
   "metadata": {},
   "source": [
    "23 REPEATED QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f227d",
   "metadata": {},
   "source": [
    "24 REPEATED QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e772c94",
   "metadata": {},
   "source": [
    "25 Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af41fc",
   "metadata": {},
   "source": [
    "SOLUTION 25\n",
    "\n",
    "Data encoding in Python is the process of converting string data into a series of bytes for efficient storage. Python has built-in functions and modules to perform encoding and decoding operations. \n",
    "\n",
    "Here are some examples of data encoding in Python: \n",
    "\n",
    "Target encoding: Replaces a category variable with its mean value. This method is also known as mean encoding. \n",
    "\n",
    "One-hot encoding: Creates a new binary column for each unique product. \n",
    "\n",
    "Label encoding: Assigns an integer value to each unique region name. \n",
    "\n",
    "Data encoding is an important step in preparing categorical data for use in machine learning algorithms. Categorical data, also known as nominal or ordinal data, represents qualitative or descriptive characteristics. Most machine learning models require numerical inputs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
